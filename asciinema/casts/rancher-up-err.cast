{"version": 2, "width": 125, "height": 33, "timestamp": 1707155222, "env": {"SHELL": "/bin/bash", "TERM": "screen-256color"}}
[0.006153, "o", "\u001b[?1049h\u001b[?1h\u001b=\u001b[H\u001b[J\u001b[34h\u001b[?25h\u001b[?1000l\u001b[?1002l\u001b[?1006l\u001b[?1005l\u001b[c\u001b[m\u000f\u001b[34l\u001b[34h\u001b[?25h\u001b[?1003l\u001b[?1006l\u001b[?2004l\u001b[1;1H\u001b[1;33r\u001b[32;19H\u001b[?1006h\u001b[?1002h\u001b[?25l\u001b[36m\u001b[HINFO\u001b[39m[0058] [worker] Building up Worker Plane..          \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] Finding container [service-sidekick] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] [sidekick] Sidekick container already created on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] [healthcheck] Start Healthcheck on service [kubelet] on host [10.3.3.151] \u001b[K\u001b[31m\r\nERRO\u001b[39m[0113] Failed to upgrade worker components on NotReady hosts, error: [Failed to verify healthcheck: Failed to check http://localhost:10248/healthz for service [kubelet] on host [10.3.3.151]: Get \"http://localhost:10248/healthz\": Unable to access the service on localhost:10248. The service might be still starting up. Error: ssh: rejected: connect failed (Connection refused), log: time=\"2024-02-05T17:34:02Z\" level=info msg=\"Start cri-dockerd grpc backend\"] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0113] [controlplan"]
[0.006756, "o", "e] Now checking status of node 10.3.3.151, try #1 \u001b[K\u001b[31m\r\nERRO\u001b[39m[0138] Host 10.3.3.151 failed to report Ready status with error: [controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found \u001b[K\u001b[36m\r\nINFO\u001b[39m[0138] [controlplane] Processing controlplane hosts for upgrade 1 at a time \u001b[K\u001b[36m\r\nINFO\u001b[39m[0138] Processing controlplane host 10.3.3.151      \u001b[K\u001b[36m\r\nINFO\u001b[39m[0138] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[K\u001b[31m\r\nERRO\u001b[39m[0163] Failed to upgrade hosts: 10.3.3.151 with error [[controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found] \u001b[K\u001b[31m\r\nFATA\u001b[39m[0163] [controlPlane] Failed to upgrade Control Plane: [[[controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found]] \u001b[K\u001b[32m\u001b[1m\r\naida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ docker ps\u001b[K\r\nCONTAINER ID   IMAGE                                 COMMAND                  CREATED         STATUS         PORTS     NAMES\u001b[K\r\ne23e8a52ec34   rancher/rke-tools:v0.1.96             \"/docker-entrypoint.…\"   3"]
[0.006802, "o", " minutes ago   Up 3 minutes             etcd-rolling-snapshots\u001b[K\r\n0c480c484bdd   rancher/hyperkube:v1.27.8-rancher2    \"/opt/rke-tools/entr…\"   16 hours ago    Up 7 seconds             kubelet\u001b[K\r\n4562ab10f3aa   rancher/hyperkube:v1.27.8-rancher2    \"/opt/rke-tools/entr…\"   16 hours ago    Up 16 hours              kube-scheduler\u001b[K\r\n645ada19db35   rancher/hyperkube:v1.27.8-rancher2    \"/opt/rke-tools/entr…\"   16 hours ago    Up 16 hours              kube-controller-manager\u001b[K\r\nda2eadafa2e3   rancher/hyperkube:v1.27.8-rancher2    \"/opt/rke-tools/entr…\"   16 hours ago    Up 3 minutes             kube-apiserver\u001b[K\r\nf7767db28e41   rancher/mirrored-coreos-etcd:v3.5.9   \"/usr/local/bin/etcd…\"   16 hours ago    Up 16 hours              etcd\u001b[K\u001b[32m\u001b[1m\r\naida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud*Z 2:AIDA-Edge-                                                                \"masterk8s\" 17:47 05-Feb-24\u001b[m\u000f\u001b[32;19H\u001b[34h\u001b[?25h\u001b[m\u000f\u001b[34l\u001b[34h\u001b[?25h\u001b[?1003l\u001b[?1006l\u001b[?2004l\u001b[1;1H\u001b[1;33r\u001b"]
[0.006863, "o", "[32;19H\u001b[?1006h\u001b[?1002h\u001b[?25l\u001b[36m\u001b[HINFO\u001b[39m[0058] [worker] Building up Worker Plane..          \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] Finding container [service-sidekick] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] [sidekick] Sidekick container already created on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] [healthcheck] Start Healthcheck on service [kubelet] on host [10.3.3.151] \u001b[K\u001b[31m\r\nERRO\u001b[39m[0113] Failed to upgrade worker components on NotReady hosts, error: [Failed to verify healthcheck: Failed to check http://localhost:10248/healthz for service [kubelet] on host [10.3.3.151]: Get \"http://localhost:10248/healthz\": Unable to access the service on localhost:10248. The service might be still starting up. Error: ssh: rejected: connect failed (Connection refused), log: time=\"2024-02-05T17:34:02Z\" level=info msg=\"Start cri-dockerd grpc backend\"] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0113] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[K\u001b[31m\r\nERRO\u001b[39m[0138] Host 10.3.3.151 failed to report Ready status wit"]
[0.006887, "o", "h error: [controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found \u001b[K\u001b[36m\r\nINFO\u001b[39m[0138] [controlplane] Processing controlplane hosts for upgrade 1 at a time \u001b[K\u001b[36m\r\nINFO\u001b[39m[0138] Processing controlplane host 10.3.3.151      \u001b[K\u001b[36m\r\nINFO\u001b[39m[0138] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[K\u001b[31m\r\nERRO\u001b[39m[0163] Failed to upgrade hosts: 10.3.3.151 with error [[controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found] \u001b[K\u001b[31m\r\nFATA\u001b[39m[0163] [controlPlane] Failed to upgrade Control Plane: [[[controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found]] \u001b[K\u001b[32m\u001b[1m\r\naida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ docker ps\u001b[K\r\nCONTAINER ID   IMAGE                                 COMMAND                  CREATED         STATUS         PORTS     NAMES\u001b[K\r\ne23e8a52ec34   rancher/rke-tools:v0.1.96             \"/docker-entrypoint.…\"   3 minutes ago   Up 3 minutes             etcd-rolling-snapshots\u001b[K\r\n0c480c484bdd   rancher/hyperkube:v1.27.8-rancher2    \"/opt"]
[0.006932, "o", "/rke-tools/entr…\"   16 hours ago    Up 7 seconds             kubelet\u001b[K\r\n4562ab10f3aa   rancher/hyperkube:v1.27.8-rancher2    \"/opt/rke-tools/entr…\"   16 hours ago    Up 16 hours              kube-scheduler\u001b[K\r\n645ada19db35   rancher/hyperkube:v1.27.8-rancher2    \"/opt/rke-tools/entr…\"   16 hours ago    Up 16 hours              kube-controller-manager\u001b[K\r\nda2eadafa2e3   rancher/hyperkube:v1.27.8-rancher2    \"/opt/rke-tools/entr…\"   16 hours ago    Up 3 minutes             kube-apiserver\u001b[K\r\nf7767db28e41   rancher/mirrored-coreos-etcd:v3.5.9   \"/usr/local/bin/etcd…\"   16 hours ago    Up 16 hours              etcd\u001b[K\u001b[32m\u001b[1m\r\naida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud*Z 2:AIDA-Edge-                                                                \"masterk8s\" 17:47 05-Feb-24\u001b[m\u000f\u001b[32;19H\u001b[34h\u001b[?25h"]
[4.052416, "o", "\u001b[1;32r\u001b[2;32r\u001b[31S\u001b[1;1H\u001b[K\u001b[32m\u001b[1maida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[1;33r\u001b[1;19H"]
[4.957381, "o", "docker ps"]
[5.302278, "o", "\u001b[9Drke up --ignore-docker-version"]
[7.90048, "o", "\r\n"]
[7.92999, "o", "\u001b[36mINFO\u001b[39m[0000] Running RKE version: v1.5.3                  \r\n"]
[7.935028, "o", "\u001b[36mINFO\u001b[39m[0000] Initiating Kubernetes cluster                \r\n"]
[8.090075, "o", "\u001b[36mINFO\u001b[39m[0000] [certificates] GenerateServingCertificate is disabled, checking if there are unused kubelet certificates \r\n\u001b[36mINFO\u001b[39m[0000] [certificates] Generating admin certificates and kubeconfig \r\n"]
[8.094058, "o", "\u001b[36mINFO\u001b[39m[0000] Successfully Deployed state file at [./cluster.rkestate] \r\n"]
[8.103079, "o", "\u001b[36mINFO\u001b[39m[0000] Building Kubernetes cluster                  \r\n\u001b[36mINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.3.13]   \r\n"]
[8.103239, "o", "\u001b[36mINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.1.253]  \r\n"]
[8.10332, "o", "\u001b[36mINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.1.91]   \r\n"]
[8.104659, "o", "\u001b[36mINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.3.151]  \r\n"]
[8.677729, "o", "\u001b[36mINFO\u001b[39m[0000] [network] No hosts added existing cluster, skipping port check \r\n"]
[8.678048, "o", "\u001b[36mINFO\u001b[39m[0000] [certificates] Deploying kubernetes certificates to Cluster nodes \r\n"]
[8.678252, "o", "\u001b[36mINFO\u001b[39m[0000] Finding container [cert-deployer] on host [10.3.1.91], try #1 \r\n\u001b[36mINFO\u001b[39m[0000] Finding container [cert-deployer] on host [10.3.3.151], try #1 \r\n\u001b[36mINFO\u001b[39m[0000] Finding container [cert-deployer] on host [10.3.1.253], try #1 "]
[8.678464, "o", "\r\n\u001b[36mINFO\u001b[39m[0000] Finding container [cert-deployer] on host [10.3.3.13], try #1 "]
[8.678646, "o", "\r\n"]
[8.683702, "o", "\u001b[36mINFO\u001b[39m[0000] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.13] \r\n"]
[8.68456, "o", "\u001b[36mINFO\u001b[39m[0000] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.1.253] \r\n"]
[8.684881, "o", "\u001b[36mINFO\u001b[39m[0000] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.1.91] \r\n"]
[8.685053, "o", "\u001b[36mINFO\u001b[39m[0000] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \r\n"]
[9.758056, "o", "\u001b[36mINFO\u001b[39m[0001] Starting container [cert-deployer] on host [10.3.1.253], try #1 \r\n"]
[9.858559, "o", "\u001b[36mINFO\u001b[39m[0001] Starting container [cert-deployer] on host [10.3.1.91], try #1 \r\n"]
[10.183731, "o", "\u001b[36mINFO\u001b[39m[0002] Starting container [cert-deployer] on host [10.3.3.13], try #1 \r\n"]
[10.195849, "o", "\u001b[36mINFO\u001b[39m[0002] Finding container [cert-deployer] on host [10.3.1.253], try #1 \r\n"]
[10.306716, "o", "\u001b[36mINFO\u001b[39m[0002] Finding container [cert-deployer] on host [10.3.1.91], try #1 \r\n"]
[10.388666, "o", "\u001b[36mINFO\u001b[39m[0002] Starting container [cert-deployer] on host [10.3.3.151], try #1 \r\n"]
[10.553323, "o", "\u001b[36mINFO\u001b[39m[0002] Finding container [cert-deployer] on host [10.3.3.13], try #1 \r\n"]
[10.6179, "o", "\u001b[36mINFO\u001b[39m[0002] Finding container [cert-deployer] on host [10.3.3.151], try #1 \r\n"]
[15.200573, "o", "\u001b[36mINFO\u001b[39m[0007] Finding container [cert-deployer] on host [10.3.1.253], try #1 \r\n"]
[15.203018, "o", "\u001b[36mINFO\u001b[39m[0007] Removing container [cert-deployer] on host [10.3.1.253], try #1 \r\n"]
[15.310539, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Finding container [cert-deployer] on host [10.3.1.91], try #1 \u001b[1;33r\u001b[32;1H"]
[15.312598, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Removing container [cert-deployer] on host [10.3.1.91], try #1 \u001b[1;33r\u001b[32;1H"]
[15.557009, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Finding container [cert-deployer] on host [10.3.3.13], try #1 \u001b[1;33r\u001b[32;1H"]
[15.559896, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Removing container [cert-deployer] on host [10.3.3.13], try #1 \u001b[1;33r\u001b[32;1H"]
[15.625256, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Finding container [cert-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[15.628302, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Removing container [cert-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[15.647352, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] [reconcile] Rebuilding and updating local kube config \u001b[1;33r\u001b[32;1H"]
[15.647626, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Successfully Deployed local admin kubeconfig at [./kube_config_cluster.yml] \u001b[1;33r\u001b[32;1H"]
[15.657727, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] [reconcile] host [10.3.3.151] is a control plane node with reachable Kubernetes API endpoint in the cluster \r\n\u001b[36mINFO\u001b[39m[0007] [certificates] Successfully deployed kubernetes certificates to Cluster nodes \u001b[1;33r\u001b[32;90H"]
[15.657918, "o", "\u001b[1;32r\u001b[32;1H\n\u001b[1;33r\u001b[32;1H"]
[15.658534, "o", "\u001b[36mINFO\u001b[39m[0007] [file-deploy] Deploying file [/etc/kubernetes/admission.yaml] to node [10.3.3.151] "]
[15.658791, "o", "\u001b[1;32r\u001b[32;1H\n\u001b[1;33r\u001b[32;1H"]
[15.662004, "o", "\u001b[36mINFO\u001b[39m[0007] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] "]
[15.662255, "o", "\u001b[1;32r\u001b[32;1H\n\u001b[1;33r\u001b[32;1H"]
[16.144526, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0008] Starting container [file-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[16.347839, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0008] Successfully started [file-deployer] container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0008] Waiting for [file-deployer] container to exit on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0008] Waiting for [file-deployer] container to exit on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[16.350465, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0008] Container [file-deployer] is still running on host [10.3.3.151]: stderr: [], stdout: [] \u001b[1;33r\u001b[32;1H"]
[17.353056, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0009] Removing container [file-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[17.371163, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0009] [remove/file-deployer] Successfully removed container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0009] [/etc/kubernetes/admission.yaml] Successfully deployed admission control config to Cluster control nodes \u001b[1;33r\u001b[32;1H"]
[17.371389, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0009] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[17.374062, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0009] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[17.899762, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0009] Starting container [file-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[18.126471, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0010] Successfully started [file-deployer] container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0010] Waiting for [file-deployer] container to exit on host [10.3.3.151] \u001b[1;33r\u001b[32;1H\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0010] Waiting for [file-deployer] container to exit on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[18.129534, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0010] Container [file-deployer] is still running on host [10.3.3.151]: stderr: [], stdout: [] \u001b[1;33r\u001b[32;1H"]
[19.132058, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0011] Removing container [file-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[19.15204, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0011] [remove/file-deployer] Successfully removed container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0011] [/etc/kubernetes/audit-policy.yaml] Successfully deployed audit policy file to Cluster control nodes \u001b[1;33r\u001b[32;1H"]
[19.152432, "o", "\u001b[36mINFO\u001b[39m[0011] [reconcile] Reconciling cluster state        "]
[19.152665, "o", "\u001b[1;32r\u001b[32;1H\n\u001b[1;33r\u001b[32;1H"]
[19.15378, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0011] [reconcile] Check etcd hosts to be deleted   \u001b[1;33r\u001b[32;1H"]
[19.1545, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0011] [reconcile] Check etcd hosts to be added     \u001b[1;33r\u001b[32;1H"]
[19.155337, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0011] [reconcile] Rebuilding and updating local kube config \u001b[1;33r\u001b[32;1H"]
[19.155767, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0011] Successfully Deployed local admin kubeconfig at [./kube_config_cluster.yml] \u001b[1;33r\u001b[32;1H"]
[19.15789, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0011] [reconcile] host [10.3.3.151] is a control plane node with reachable Kubernetes API endpoint in the cluster \r\n\u001b[36mINFO\u001b[39m[0011] [reconcile] Reconciled cluster state successfully \u001b[1;33r\u001b[32;1H"]
[19.158059, "o", "\u001b[36mINFO\u001b[39m[0011] max_unavailable_worker got rounded down to 0, resetting to 1 "]
[19.15834, "o", "\u001b[1;32r\u001b[32;1H\n\u001b[1;33r\u001b[32;1H"]
[19.158478, "o", "\u001b[36mINFO\u001b[39m[0011] Setting maxUnavailable for worker nodes to: 1 "]
[19.158737, "o", "\u001b[1;32r\u001b[2S\u001b[31d\u001b[36mINFO\u001b[39m[0011] Setting maxUnavailable for controlplane nodes to: 1 \u001b[1;33r\u001b[32;1H"]
[19.160123, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0011] Restarting container [kube-apiserver] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[24.392483, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0016] [restart/kube-apiserver] Successfully restarted container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0016] Pre-pulling kubernetes images                \u001b[1;33r\u001b[32;1H"]
[24.393833, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0016] Image [rancher/hyperkube:v1.27.8-rancher2] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[24.394056, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0016] Image [rancher/hyperkube:v1.27.8-rancher2] exists on host [10.3.3.13] \u001b[1;33r\u001b[32;1H"]
[24.395041, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0016] Image [rancher/hyperkube:v1.27.8-rancher2] exists on host [10.3.1.253] \r\n\u001b[36mINFO\u001b[39m[0016] Image [rancher/hyperkube:v1.27.8-rancher2] exists on host [10.3.1.91] \r\n\u001b[36mINFO\u001b[39m[0016] Image [rancher/mirrored-pause:3.7] exists on host [10.3.3.13] \u001b[1;33r\u001b[32;1H"]
[24.395266, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0016] Image [rancher/mirrored-pause:3.7] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[24.395805, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0016] Image [rancher/mirrored-pause:3.7] exists on host [10.3.1.253] \u001b[1;33r\u001b[32;1H"]
[24.395962, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0016] Image [rancher/mirrored-pause:3.7] exists on host [10.3.1.91] \r\n\u001b[36mINFO\u001b[39m[0016] Kubernetes images pulled successfully        \u001b[1;33r\u001b[32;1H"]
[24.397163, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0016] [etcd] Building up etcd plane..              \u001b[1;33r\u001b[32;1H"]
[24.399282, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0016] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[24.976962, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0017] Starting container [etcd-fix-perm] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[25.219387, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0017] Successfully started [etcd-fix-perm] container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0017] Waiting for [etcd-fix-perm] container to exit on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0017] Waiting for [etcd-fix-perm] container to exit on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[25.221514, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0017] Container [etcd-fix-perm] is still running on host [10.3.3.151]: stderr: [], stdout: [] \u001b[1;33r\u001b[32;1H"]
[26.224856, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0018] Removing container [etcd-fix-perm] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[26.243552, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0018] [remove/etcd-fix-perm] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[26.247123, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0018] [etcd] Running rolling snapshot container [etcd-rolling-snapshots] on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[26.24784, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0018] Removing container [etcd-rolling-snapshots] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[26.333303, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0018] [remove/etcd-rolling-snapshots] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[26.335491, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0018] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[26.901031, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0018] Starting container [etcd-rolling-snapshots] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[27.004632, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0019] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[32.009912, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0024] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[32.569783, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0024] Starting container [rke-bundle-cert] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[32.766494, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0024] [certificates] Successfully started [rke-bundle-cert] container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0024] Waiting for [rke-bundle-cert] container to exit on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[32.768422, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0024] Container [rke-bundle-cert] is still running on host [10.3.3.151]: stderr: [], stdout: [] \u001b[1;33r\u001b[32;1H"]
[33.770182, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0025] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0025] Removing container [rke-bundle-cert] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[33.792007, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0025] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[34.338111, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0026] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[34.544469, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0026] [etcd] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[34.545877, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0026] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[34.711249, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0026] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[34.714953, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0026] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[34.834641, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0026] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[35.562338, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0027] [etcd] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[35.56363, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0027] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[35.769041, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0027] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0027] [etcd] Successfully started etcd plane.. Checking etcd cluster health \u001b[1;33r\u001b[32;1H"]
[36.315488, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0028] [etcd] etcd host [10.3.3.151] reported healthy=true \u001b[1;33r\u001b[32;1H\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0028] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[1;33r\u001b[32;1H"]
[60.030813, "o", "\u001b[?25l\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud*Z 2:AIDA-Edge-                                                                \"masterk8s\" 17:48 05-Feb-24\u001b[m\u000f\u001b[32;1H\u001b[34h\u001b[?25h"]
[61.348653, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0053] Attempting upgrade of controlplane components on following hosts in NotReady status: 10.3.3.151 \r\n\u001b[36mINFO\u001b[39m[0053] [controlplane] Building up Controller Plane.. \u001b[1;33r\u001b[32;1H\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0053] Finding container [service-sidekick] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[61.352732, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0053] [sidekick] Sidekick container already created on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[61.355521, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0053] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[61.857256, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0053] [healthcheck] service [kube-apiserver] on host [10.3.3.151] is healthy \u001b[1;33r\u001b[32;1H"]
[61.861108, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0053] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[62.354661, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0054] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[62.582826, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0054] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[62.584179, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0054] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[62.741593, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0054] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[62.74613, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0054] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[63.245654, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0055] [healthcheck] service [kube-controller-manager] on host [10.3.3.151] is healthy \u001b[1;33r\u001b[32;1H"]
[63.248973, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0055] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[63.759082, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0055] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[63.961883, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0056] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[63.962965, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0056] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[64.108215, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0056] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[64.113069, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0056] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[64.625191, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0056] [healthcheck] service [kube-scheduler] on host [10.3.3.151] is healthy \u001b[1;33r\u001b[32;1H"]
[64.628876, "o", "\u001b[36mINFO\u001b[39m[0056] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] "]
[64.629054, "o", "\u001b[1;32r\u001b[32;1H\n\u001b[1;33r\u001b[32;1H"]
[65.136201, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0057] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[65.34095, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0057] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[65.342044, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0057] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[65.501294, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0057] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0057] [controlplane] Successfully started Controller Plane.. \r\n\u001b[36mINFO\u001b[39m[0057] [worker] Building up Worker Plane..          \r\n\u001b[36mINFO\u001b[39m[0057] Finding container [service-sidekick] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[65.514784, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0057] [sidekick] Sidekick container already created on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[65.518301, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0057] [healthcheck] Start Healthcheck on service [kubelet] on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[120.076506, "o", "\u001b[?25l\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud*Z 2:AIDA-Edge-                                                                \"masterk8s\" 17:49 05-Feb-24\u001b[m\u000f\u001b[32;1H\u001b[34h\u001b[?25h"]
[120.986001, "o", "\u001b[1;32r\u001b[5S\u001b[27d\u001b[31mERRO\u001b[39m[0113] Failed to upgrade worker components on NotReady hosts, error: [Failed to verify healthcheck: Failed to check http://localhost:10248/healthz for service [kubelet] on host [10.3.3.151]: Get \"http://localhost:10248/healthz\": Unable to access the service on localhost:10248. The service might be still starting up. Error: ssh: rejected: connect failed (Connection refused), log: time=\"2024-02-05T17:48:54Z\" level=info msg=\"Start cri-dockerd grpc backend\"] \r\n\u001b[36mINFO\u001b[39m[0113] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[1;33r\u001b[32;1H"]
[146.011487, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[31mERRO\u001b[39m[0138] Host 10.3.3.151 failed to report Ready status with error: [controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found \r\n\u001b[36mINFO\u001b[39m[0138] [controlplane] Processing controlplane hosts for upgrade 1 at a time \u001b[1;33r\u001b[32;1H\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0138] Processing controlplane host 10.3.3.151      \r\n\u001b[36mINFO\u001b[39m[0138] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[1;33r\u001b[32;1H"]
[171.03398, "o", "\u001b[1;32r\u001b[4S\u001b[28d\u001b[31mERRO\u001b[39m[0163] Failed to upgrade hosts: 10.3.3.151 with error [[controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found] \r\n\u001b[31mFATA\u001b[39m[0163] [controlPlane] Failed to upgrade Control Plane: [[[controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found]] \u001b[1;33r\u001b[32;1H"]
[171.05234, "o", "\u001b[32m\u001b[1maida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ "]
[180.107258, "o", "\u001b[?25l\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud*Z 2:AIDA-Edge-                                                                \"masterk8s\" 17:50 05-Feb-24\u001b[m\u000f\u001b[32;19H\u001b[34h\u001b[?25h"]
[240.148852, "o", "\u001b[?25l\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud*Z 2:AIDA-Edge-                                                                \"masterk8s\" 17:51 05-Feb-24\u001b[m\u000f\u001b[32;19H\u001b[34h\u001b[?25h"]
[293.316401, "o", "\u001b[32m\u001b[8;1H─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[39m\u001b[16;1H─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[24;1H─────────────────────────────────────────────────────────────────────────────────"]
[293.316464, "o", "────────────────────────────────────────────\u001b[?25l\u001b[m\u000f\u001b[36m\u001b[1;1HINFO\u001b[39m[0138] Processing controlplane host 10.3.3.151      \u001b[K\u001b[36m\r\nINFO\u001b[39m[0138] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[K\u001b[31m\r\nERRO\u001b[39m[0163] Failed to upgrade hosts: 10.3.3.151 with error [[controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found] \u001b[K\u001b[31m\r\nFATA\u001b[39m[0163] [controlPlane] Failed to upgrade Control Plane: [[[controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found]] \u001b[K\u001b[32m\u001b[1m\r\naida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\u001b[32m\u001b[1m\u001b[9;1Haida@worker01\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ docker ps\u001b[K\r\nCONTAINER ID   IMAGE                                COMMAND                  CREATED        STATUS         PORTS     NAMES\u001b[K\r\n4a08540702bc   rancher/hyperkube:v1.27.8-rancher2   \"/opt/rke-tools/entr…\"   16 hours ago   Up 9 seconds             kubelet\u001b[K\r\nbc18660f091f   rancher/rke-tools:v0.1.96            \"ngin"]
[293.316477, "o", "x-proxy CP_HOST…\"   16 hours ago   Up 16 hours              nginx-proxy\u001b[K\u001b[32m\u001b[1m\r\naida@worker01\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\r\n\u001b[K\u001b[32m\u001b[1m\u001b[2Baida@worker02\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\u001b[32m\u001b[1m\u001b[2Baida@worker03\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud* 2:AIDA-Edge-                                                                 \"masterk8s\" 17:51 05-Feb-24\u001b[m\u000f\u001b[7;19H\u001b[34h\u001b[?25h\r\u001b[K\u001b[32m\u001b[1maida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ "]
[294.650669, "o", "s"]
[294.777992, "o", "e"]
[294.890798, "o", "n"]
[294.96249, "o", "d"]
[295.077325, "o", "-"]
[295.261297, "o", "t"]
[295.340449, "o", "m"]
[295.540072, "o", "u"]
[295.620835, "o", "x"]
[295.944403, "o", " "]
[296.14529, "o", "c"]
[296.227317, "o", "l"]
[296.331327, "o", "e"]
[296.858609, "o", "\u001b[3D\u001b[K"]
[297.161193, "o", "c"]
[297.232213, "o", "l"]
[297.385226, "o", "o"]
[297.466285, "o", "u"]
[297.536072, "o", "d"]
[297.569211, "o", " "]
[297.753183, "o", "c"]
[297.856065, "o", "l"]
[297.958408, "o", "e"]
[298.079158, "o", "a"]
[298.16158, "o", "r"]
[298.345557, "o", "\u001b[1;7r\u001b[7;1H\n\u001b[1;33r\u001b[7;1H"]
[298.351277, "o", "\u001b[1;7r\u001b[7;1H\n\u001bMclear\u001b[1;33r\u001b[7;1H"]
[298.356474, "o", "\u001b[17;18Hclear\u001b[7;1H"]
[298.357215, "o", "\u001b[14;18Hclear\u001b[7;1H"]
[298.357727, "o", "\u001b[17;23r\u001b[18;23r\u001b[6S\u001b[17;1H\u001b[K\u001b[1;33r\u001b[7;1H"]
[298.358018, "o", "\u001b[10B\u001b[32m\u001b[1maida@worker02\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[7;1H"]
[298.358423, "o", "\u001b[9;15r\u001b[10;15r\u001b[6S\u001b[9;1H\u001b[K\u001b[1;33r\u001b[7;1H"]
[298.358643, "o", "\u001b[2B\u001b[32m\u001b[1maida@worker01\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[7;1H"]
[298.359784, "o", "\u001b[32m\u001b[1maida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[1;7r\u001b[7;1H\n\u001b[6;19Hclear\u001b[1;33r\u001b[7;1H"]
[298.36055, "o", "\u001b[25;18Hclear\u001b[7;1H"]
[298.361435, "o", "\u001b[1;7r\u001b[2;7r\u001b[6S\u001b[1;1H\u001b[K\u001b[1;33r\u001b[1;1H"]
[298.36164, "o", "\u001b[25;32r\u001b[26;32r\u001b[7S\u001b[25;1H\u001b[K\u001b[1;33r\u001b[1;1H"]
[298.361839, "o", "\u001b[32m\u001b[1maida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[25;1H\u001b[32m\u001b[1maida@worker03\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[1;19H"]
[298.797435, "o", "clear"]
[299.427767, "o", "\u001b[5Dsend-tmux cloud clear"]
[299.929115, "o", "\u001b[5D\u001b[K"]
[300.172461, "o", "\u001b[6D\u001b[K"]
[300.181099, "o", "\u001b[?25l\u001b[32m\u001b[40m\u001b[33;1H[AIDA] 1:AIDA-Cloud* 2:AIDA-Edge-                                                                 \"masterk8s\" 17:52 05-Feb-24\u001b[m\u000f\u001b[1;29H\u001b[34h\u001b[?25h"]
[300.768399, "o", "c"]
[300.839554, "o", "l"]
[301.015938, "o", "o"]
[301.086529, "o", "u"]
[301.157269, "o", "d"]
[301.195098, "o", " "]
[301.314416, "o", "'"]
[301.801117, "o", "i"]
[301.872276, "o", "p"]
[302.123416, "o", " "]
[302.20612, "o", "-"]
[302.349451, "o", "b"]
[302.554269, "o", "r"]
[302.778412, "o", "i"]
[302.899388, "o", "e"]
[303.001395, "o", "f"]
[303.248634, "o", " "]
[303.452527, "o", "a"]
[303.524598, "o", "d"]
[303.63633, "o", "d"]
[303.920299, "o", "'"]
[304.268294, "o", "\r\n"]
[304.272329, "o", "ip -brief add\r\n"]
[304.275381, "o", "\u001b[9;18Hip -brief add\u001b[3;1H"]
[304.279009, "o", "\u001b[7B\u001b[36mlo               \u001b[39mUNKNOWN        \u001b[35m127.0.0.1\u001b[39m/8 \u001b[34m::1\u001b[39m/128 \r\n\u001b[36meth0             \u001b[32mUP             \u001b[35m10.3.1.91\u001b[39m/16 \u001b[34mfe80::108f:59ff:feca:dc3c\u001b[39m/64 \r\n\u001b[36mdocker0          \u001b[31mDOWN           \u001b[35m172.17.0.1\u001b[39m/16 \u001b[34mfe80::42:f6ff:fee6:ae44\u001b[39m/64 \r\n\u001b[32m\u001b[1maida@worker01\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[4Bip -brief add\u001b[3;1H"]
[304.280179, "o", "\u001b[15B\u001b[36mlo               \u001b[39mUNKNOWN        \u001b[35m127.0.0.1\u001b[39m/8 \u001b[34m::1\u001b[39m/128 \r\n\u001b[36meth0             \u001b[32mUP             \u001b[35m10.3.1.253\u001b[39m/16 \u001b[34mfe80::3867:a7ff:fe51:b690\u001b[39m/64 \r\n\u001b[36mdocker0          \u001b[31mDOWN           \u001b[35m172.17.0.1\u001b[39m/16 \u001b[34mfe80::42:55ff:fee6:a14b\u001b[39m/64 \u001b[3;1H"]
[304.280587, "o", "\u001b[21d\u001b[32m\u001b[1maida@worker02\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[3;1H"]
[304.282865, "o", "\u001b[32m\u001b[1maida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ ip -brief add\r\n\u001b[25;18Hip -brief add"]
[304.283111, "o", "\u001b[4;1H"]
[304.284507, "o", "\u001b[26d\u001b[36mlo               \u001b[39mUNKNOWN        \u001b[35m127.0.0.1\u001b[39m/8 \u001b[34m::1\u001b[39m/128 \r\n\u001b[36meth0             \u001b[32mUP             \u001b[35m10.3.3.13\u001b[39m/16 \u001b[34mfe80::e08b:3ff:fec0:4371\u001b[39m/64 \r\n\u001b[36mdocker0          \u001b[31mDOWN           \u001b[35m172.17.0.1\u001b[39m/16 \u001b[34mfe80::42:cbff:fe9c:f9be\u001b[39m/64 \r\n\u001b[32m\u001b[1maida@worker03\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[4;1H"]
[304.28509, "o", "\u001b[36mlo               \u001b[39mUNKNOWN        \u001b[35m127.0.0.1\u001b[39m/8 \u001b[34m::1\u001b[39m/128 \r\n\u001b[36meth0             \u001b[32mUP             \u001b[35m10.3.3.151\u001b[39m/16 \u001b[34mfe80::602f:6eff:fe9f:a7dd\u001b[39m/64 \r\n\u001b[36mdocker0          \u001b[31mDOWN           \u001b[35m172.17.0.1\u001b[39m/16 \u001b[34mfe80::42:36ff:fe81:1bac\u001b[39m/64 \r\n"]
[304.285314, "o", "\u001b[32m\u001b[1maida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ "]
[316.834975, "o", "\u001b[1;33r\u001b[m\u000f\u001b[?1l\u001b>\u001b[H\u001b[J\u001b[34h\u001b[?25h\u001b[?1000l\u001b[?1002l\u001b[?1006l\u001b[?1005l\u001b[?1049l"]
[316.83503, "o", "[detached (from session AIDA)]\r\n"]
[316.841126, "o", "\u001b[?1049h\u001b[?1h\u001b=\u001b[H\u001b[J\u001b[34h\u001b[?25h\u001b[?1000l\u001b[?1002l\u001b[?1006l\u001b[?1005l\u001b[c\u001b[m\u000f\u001b[34l\u001b[34h\u001b[?25h\u001b[?1003l\u001b[?1006l\u001b[?2004l\u001b[1;1H\u001b[1;33r\u001b[7;19H\u001b[?1006h\u001b[?1002h"]
[316.842029, "o", "\u001b[32m\r\n─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[39m\u001b[16;1H─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[24;1H──────────────────────────────────────────────────────────────────────────────────"]
[316.842079, "o", "───────────────────────────────────────────\u001b[?25l\u001b[m\u000f\u001b[32m\u001b[1m\u001b[1;1Haida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ send-tmux cloud 'ip -brief add'\u001b[K\r\nip -brief add\u001b[K\u001b[32m\u001b[1m\r\naida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ ip -brief add\u001b[K\u001b[36m\r\nlo               \u001b[39mUNKNOWN        \u001b[35m127.0.0.1\u001b[39m/8 \u001b[34m::1\u001b[39m/128 \u001b[K\u001b[36m\r\neth0             \u001b[32mUP             \u001b[35m10.3.3.151\u001b[39m/16 \u001b[34mfe80::602f:6eff:fe9f:a7dd\u001b[39m/64 \u001b[K\u001b[36m\r\ndocker0          \u001b[31mDOWN           \u001b[35m172.17.0.1\u001b[39m/16 \u001b[34mfe80::42:36ff:fe81:1bac\u001b[39m/64 \u001b[K\u001b[32m\u001b[1m\r\naida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\u001b[32m\u001b[1m\u001b[9;1Haida@worker01\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ ip -brief add\u001b[K\u001b[36m\r\nlo               \u001b[39mUNKNOWN        \u001b[35m127.0.0.1\u001b[39m/8 \u001b[34m::1\u001b[39m/128 \u001b[K\u001b[36m\r\neth0             \u001b[32mUP             \u001b[35m10.3.1.91\u001b[39m/16 \u001b[34mfe80::108f:59ff:feca:dc3c\u001b[39m/64 \u001b[K\u001b[36m\r\ndocker0          \u001b[31mDOWN           \u001b[35m172.17.0.1\u001b[39m/16 \u001b[34mfe80::42:f6ff:fee6:ae44\u001b[39m/64 \u001b[K"]
[316.842095, "o", "\u001b[32m\u001b[1m\r\naida@worker01\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\r\n\u001b[K\r\n\u001b[K\u001b[32m\u001b[1m\u001b[2Baida@worker02\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ ip -brief add\u001b[K\u001b[36m\r\nlo               \u001b[39mUNKNOWN        \u001b[35m127.0.0.1\u001b[39m/8 \u001b[34m::1\u001b[39m/128 \u001b[K\u001b[36m\r\neth0             \u001b[32mUP             \u001b[35m10.3.1.253\u001b[39m/16 \u001b[34mfe80::3867:a7ff:fe51:b690\u001b[39m/64 \u001b[K\u001b[36m\r\ndocker0          \u001b[31mDOWN           \u001b[35m172.17.0.1\u001b[39m/16 \u001b[34mfe80::42:55ff:fee6:a14b\u001b[39m/64 \u001b[K\u001b[32m\u001b[1m\r\naida@worker02\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\r\n\u001b[K\r\n\u001b[K\u001b[32m\u001b[1m\u001b[2Baida@worker03\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ ip -brief add\u001b[K\u001b[36m\r\nlo               \u001b[39mUNKNOWN        \u001b[35m127.0.0.1\u001b[39m/8 \u001b[34m::1\u001b[39m/128 \u001b[K\u001b[36m\r\neth0             \u001b[32mUP             \u001b[35m10.3.3.13\u001b[39m/16 \u001b[34mfe80::e08b:3ff:fec0:4371\u001b[39m/64 \u001b[K\u001b[36m\r\ndocker0          \u001b[31mDOWN           \u001b[35m172.17.0.1\u001b[39m/16 \u001b[34mfe80::42:cbff:fe9c:f9be\u001b[39m/64 \u001b[K\u001b[32m\u001b[1m\r\naida@worker03\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud* 2:AIDA-Edge-                                                                 \""]
[316.842112, "o", "masterk8s\" 18:00 05-Feb-24\u001b[m\u000f\u001b[7;19H\u001b[34h\u001b[?25h\u001b[m\u000f\u001b[34l\u001b[34h\u001b[?25h\u001b[?1003l\u001b[?1006l\u001b[?2004l\u001b[1;1H\u001b[1;33r"]
[316.842438, "o", "\u001b[32m\u001b[8;1H─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[39m\u001b[16;1H─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[24;1H─────────────────────────────────────────────────────────────────────────────────"]
[316.842467, "o", "────────────────────────────────────────────\u001b[?25l\u001b[m\u000f\u001b[32m\u001b[1m\u001b[1;1Haida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ send-tmux cloud 'ip -brief add'\u001b[K\r\nip -brief add\u001b[K\u001b[32m\u001b[1m\r\naida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ ip -brief add\u001b[K\u001b[36m\r\nlo               \u001b[39mUNKNOWN        \u001b[35m127.0.0.1\u001b[39m/8 \u001b[34m::1\u001b[39m/128 \u001b[K\u001b[36m\r\neth0             \u001b[32mUP             \u001b[35m10.3.3.151\u001b[39m/16 \u001b[34mfe80::602f:6eff:fe9f:a7dd\u001b[39m/64 \u001b[K\u001b[36m\r\ndocker0          \u001b[31mDOWN           \u001b[35m172.17.0.1\u001b[39m/16 \u001b[34mfe80::42:36ff:fe81:1bac\u001b[39m/64 \u001b[K\u001b[32m\u001b[1m\r\naida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\u001b[32m\u001b[1m\u001b[9;1Haida@worker01\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ ip -brief add\u001b[K\u001b[36m\r\nlo               \u001b[39mUNKNOWN        \u001b[35m127.0.0.1\u001b[39m/8 \u001b[34m::1\u001b[39m/128 \u001b[K\u001b[36m\r\neth0             \u001b[32mUP             \u001b[35m10.3.1.91\u001b[39m/16 \u001b[34mfe80::108f:59ff:feca:dc3c\u001b[39m/64 \u001b[K\u001b[36m\r\ndocker0          \u001b[31mDOWN           \u001b[35m172.17.0.1\u001b[39m/16 \u001b[34mfe80::42:f6ff:fee6:ae44\u001b[39m/64"]
[316.842482, "o", " \u001b[K\u001b[32m\u001b[1m\r\naida@worker01\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\r\n\u001b[K\r\n\u001b[K\u001b[32m\u001b[1m\u001b[2Baida@worker02\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ ip -brief add\u001b[K\u001b[36m\r\nlo               \u001b[39mUNKNOWN        \u001b[35m127.0.0.1\u001b[39m/8 \u001b[34m::1\u001b[39m/128 \u001b[K\u001b[36m\r\neth0             \u001b[32mUP             \u001b[35m10.3.1.253\u001b[39m/16 \u001b[34mfe80::3867:a7ff:fe51:b690\u001b[39m/64 \u001b[K\u001b[36m\r\ndocker0          \u001b[31mDOWN           \u001b[35m172.17.0.1\u001b[39m/16 \u001b[34mfe80::42:55ff:fee6:a14b\u001b[39m/64 \u001b[K\u001b[32m\u001b[1m\r\naida@worker02\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\r\n\u001b[K\r\n\u001b[K\u001b[32m\u001b[1m\u001b[2Baida@worker03\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ ip -brief add\u001b[K\u001b[36m\r\nlo               \u001b[39mUNKNOWN        \u001b[35m127.0.0.1\u001b[39m/8 \u001b[34m::1\u001b[39m/128 \u001b[K\u001b[36m\r\neth0             \u001b[32mUP             \u001b[35m10.3.3.13\u001b[39m/16 \u001b[34mfe80::e08b:3ff:fec0:4371\u001b[39m/64 \u001b[K\u001b[36m\r\ndocker0          \u001b[31mDOWN           \u001b[35m172.17.0.1\u001b[39m/16 \u001b[34mfe80::42:cbff:fe9c:f9be\u001b[39m/64 \u001b[K\u001b[32m\u001b[1m\r\naida@worker03\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud* 2:AIDA-Edge-                                                              "]
[316.842497, "o", "   \"masterk8s\" 18:00 05-Feb-24\u001b[m\u000f\u001b[7;19H\u001b[34h\u001b[?25h\u001b[?1006h\u001b[?1002h"]
[318.012837, "o", "r"]
[318.30856, "o", "k"]
[318.36148, "o", "e"]
[318.445259, "o", " "]
[318.689452, "o", "u"]
[318.760668, "o", "p"]
[319.660953, "o", "\r\u001b[4@(reverse-i-search)`':\u001b[7C"]
[320.34077, "o", "\u001b[9D\u001b[1@ \u001b[6C"]
[320.421625, "o", "\u001b[6D-': ip -brief add\u001b[11D"]
[320.630773, "o", "\u001b[5D-': rke up --ignore-docker-version\u001b[24D"]
[321.677974, "o", "\r\u001b[7P\u001b[1;7r\u001b[7;1H\n\u001bM\u001b[32m\u001b[1maida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$\r\u001b[32m\u001b[1maida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[1;33r\u001b[7;1H"]
[321.722455, "o", "\u001b[36mINFO\u001b[39m[0000] Running RKE version: v1.5.3                  \u001b[1;7r\u001b[7;1H\n\u001b[1;33r\u001b[7;1H"]
[321.72852, "o", "\u001b[36mINFO\u001b[39m[0000] Initiating Kubernetes cluster                \u001b[1;7r\u001b[7;1H\n\u001b[1;33r\u001b[7;1H"]
[321.897749, "o", "\u001b[1;7r\u001b[7;1H\n\u001bM\u001b[36mINFO\u001b[39m[0000] [certificates] GenerateServingCertificate is disabled, checking if there are unused kubelet certificates \u001b[1;33r\u001b[7;1H\u001b[1;7r\u001b[7;1H\n\u001bM\u001b[36mINFO\u001b[39m[0000] [certificates] Generating admin certificates and kubeconfig \u001b[1;33r\u001b[7;1H"]
[321.902356, "o", "\u001b[1;7r\u001b[7;1H\n\u001bM\u001b[36mINFO\u001b[39m[0000] Successfully Deployed state file at [./cluster.rkestate] \u001b[1;33r\u001b[7;1H"]
[321.918957, "o", "\u001b[1;7r\u001b[7;1H\n\u001bM\u001b[36mINFO\u001b[39m[0000] Building Kubernetes cluster                  \u001b[1;33r\u001b[7;1H\u001b[1;7r\u001b[7;1H\n\u001bM\u001b[36mINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.3.13]   \u001b[1;33r\u001b[7;1H"]
[321.919078, "o", "\u001b[1;7r\u001b[7;1H\n\u001bM\u001b[36mINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.1.91]   \u001b[1;33r\u001b[7;1H"]
[321.919278, "o", "\u001b[1;7r\u001b[7;1H\n\u001bM\u001b[36mINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.1.253]  \u001b[1;33r\u001b[7;1H"]
[321.920652, "o", "\u001b[1;7r\u001b[7;1H\n\u001bM\u001b[36mINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.3.151]  \u001b[1;33r\u001b[7;1H"]
[322.131878, "o", "\u001b[?25l\u001b[32m\u001b[40m\u001b[33d[AIDA] 1:AIDA-Cloud* 2:AIDA-Edge-                                                                 \"masterk8s\" 18:01 05-Feb-24\u001b[m\u000f\u001b[7;1H\u001b[34h\u001b[?25h"]
[322.337931, "o", "\u001b[?25l\u001b[32m\u001b[1m\u001b[Haida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ send-tmux cloud 'ip -brief add'\u001b[K\r\nip -brief add\u001b[K\u001b[32m\u001b[1m\r\naida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ ip -brief add\u001b[K\u001b[36m\r\nlo               \u001b[39mUNKNOWN        \u001b[35m127.0.0.1\u001b[39m/8 \u001b[34m::1\u001b[39m/128 \u001b[K\u001b[36m\r\neth0             \u001b[32mUP             \u001b[35m10.3.3.151\u001b[39m/16 \u001b[34mfe80::602f:6eff:fe9f:a7dd\u001b[39m/64 \u001b[K\u001b[36m\r\ndocker0          \u001b[31mDOWN           \u001b[35m172.17.0.1\u001b[39m/16 \u001b[34mfe80::42:36ff:fe81:1bac\u001b[39m/64 \u001b[K\u001b[32m\u001b[1m\r\naida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ rke up --ignore-docker-version\u001b[K\u001b[36m\r\nINFO\u001b[39m[0000] Running RKE version: v1.5.3                  \u001b[K\u001b[36m\r\nINFO\u001b[39m[0000] Initiating Kubernetes cluster                \u001b[K\u001b[36m\r\nINFO\u001b[39m[0000] [certificates] GenerateServingCertificate is disabled, checking if there are unused kubelet certificates \u001b[K\u001b[36m\r\nINFO\u001b[39m[0000] [certificates] Generating admin certificates and kubeconfig \u001b[K\u001b[36m\r\nINFO\u001b[39m[0000] Successfully Deployed state file at [./cluster.rkestate] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0000] Building"]
[322.338014, "o", " Kubernetes cluster                  \u001b[K\u001b[36m\r\nINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.3.13]   \u001b[K\u001b[36m\r\nINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.1.91]   \u001b[K\u001b[36m\r\nINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.1.253]  \u001b[K\u001b[36m\r\nINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.3.151]  \u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud*Z 2:AIDA-Edge-                                                                \"masterk8s\" 18:01 05-Feb-24\u001b[m\u000f\u001b[18;1H\u001b[34h\u001b[?25h"]
[322.533865, "o", "\u001b[36mINFO\u001b[39m[0000] [network] No hosts added existing cluster, skipping port check \r\n"]
[322.533946, "o", "\u001b[36mINFO\u001b[39m[0000] [certificates] Deploying kubernetes certificates to Cluster nodes \r\n"]
[322.534279, "o", "\u001b[36mINFO\u001b[39m[0000] Finding container [cert-deployer] on host [10.3.3.13], try #1 \r\n"]
[322.534786, "o", "\u001b[36mINFO\u001b[39m[0000] Finding container [cert-deployer] on host [10.3.3.151], try #1 \r\n\u001b[36mINFO\u001b[39m[0000] Finding container [cert-deployer] on host [10.3.1.91], try #1 \r\n\u001b[36mINFO\u001b[39m[0000] Finding container [cert-deployer] on host [10.3.1.253], try #1 \r\n"]
[322.542678, "o", "\u001b[36mINFO\u001b[39m[0000] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.1.91] \r\n\u001b[36mINFO\u001b[39m[0000] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.13] \r\n\u001b[36mINFO\u001b[39m[0000] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.1.253] \r\n"]
[322.544673, "o", "\u001b[36mINFO\u001b[39m[0000] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \r\n"]
[323.959141, "o", "\u001b[36mINFO\u001b[39m[0002] Starting container [cert-deployer] on host [10.3.3.13], try #1 \r\n"]
[324.013965, "o", "\u001b[36mINFO\u001b[39m[0002] Starting container [cert-deployer] on host [10.3.1.91], try #1 \r\n"]
[324.10404, "o", "\u001b[36mINFO\u001b[39m[0002] Starting container [cert-deployer] on host [10.3.3.151], try #1 \r\n"]
[324.178904, "o", "\u001b[36mINFO\u001b[39m[0002] Starting container [cert-deployer] on host [10.3.1.253], try #1 \r\n"]
[324.217082, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0002] Finding container [cert-deployer] on host [10.3.3.13], try #1 \u001b[1;33r\u001b[32;1H"]
[324.239797, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0002] Finding container [cert-deployer] on host [10.3.1.91], try #1 \u001b[1;33r\u001b[32;1H"]
[324.335328, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0002] Finding container [cert-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[324.397868, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0002] Finding container [cert-deployer] on host [10.3.1.253], try #1 \u001b[1;33r\u001b[32;1H"]
[329.225463, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Finding container [cert-deployer] on host [10.3.3.13], try #1 \u001b[1;33r\u001b[32;1H"]
[329.228218, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Removing container [cert-deployer] on host [10.3.3.13], try #1 \u001b[1;33r\u001b[32;1H"]
[329.244289, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Finding container [cert-deployer] on host [10.3.1.91], try #1 \u001b[1;33r\u001b[32;1H"]
[329.246981, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Removing container [cert-deployer] on host [10.3.1.91], try #1 \u001b[1;33r\u001b[32;1H"]
[329.340141, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Finding container [cert-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[329.343053, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Removing container [cert-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[329.40161, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Finding container [cert-deployer] on host [10.3.1.253], try #1 \u001b[1;33r\u001b[32;1H"]
[329.403936, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Removing container [cert-deployer] on host [10.3.1.253], try #1 \u001b[1;33r\u001b[32;1H"]
[329.42246, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] [reconcile] Rebuilding and updating local kube config \u001b[1;33r\u001b[32;1H"]
[329.422793, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Successfully Deployed local admin kubeconfig at [./kube_config_cluster.yml] \u001b[1;33r\u001b[32;1H"]
[329.433592, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0007] [reconcile] host [10.3.3.151] is a control plane node with reachable Kubernetes API endpoint in the cluster \r\n\u001b[36mINFO\u001b[39m[0007] [certificates] Successfully deployed kubernetes certificates to Cluster nodes \u001b[1;33r\u001b[32;1H"]
[329.434223, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] [file-deploy] Deploying file [/etc/kubernetes/admission.yaml] to node [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[329.437414, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0007] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[329.923648, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0008] Starting container [file-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[330.147956, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0008] Successfully started [file-deployer] container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0008] Waiting for [file-deployer] container to exit on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0008] Waiting for [file-deployer] container to exit on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[330.149977, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0008] Container [file-deployer] is still running on host [10.3.3.151]: stderr: [], stdout: [] \u001b[1;33r\u001b[32;1H"]
[331.15288, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0009] Removing container [file-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[331.172188, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0009] [remove/file-deployer] Successfully removed container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0009] [/etc/kubernetes/admission.yaml] Successfully deployed admission control config to Cluster control nodes \u001b[1;33r\u001b[32;1H"]
[331.172479, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0009] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[331.175723, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0009] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[331.677465, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0009] Starting container [file-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[331.896044, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0010] Successfully started [file-deployer] container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0010] Waiting for [file-deployer] container to exit on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0010] Waiting for [file-deployer] container to exit on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[331.89825, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0010] Container [file-deployer] is still running on host [10.3.3.151]: stderr: [], stdout: [] \u001b[1;33r\u001b[32;1H"]
[332.900678, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0011] Removing container [file-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[332.923085, "o", "\u001b[1;32r\u001b[7S\u001b[25d\u001b[36mINFO\u001b[39m[0011] [remove/file-deployer] Successfully removed container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0011] [/etc/kubernetes/audit-policy.yaml] Successfully deployed audit policy file to Cluster control nodes \r\n\u001b[36mINFO\u001b[39m[0011] [reconcile] Reconciling cluster state        \r\n\u001b[36mINFO\u001b[39m[0011] [reconcile] Check etcd hosts to be deleted   \r\n\u001b[36mINFO\u001b[39m[0011] [reconcile] Check etcd hosts to be added     \r\n\u001b[36mINFO\u001b[39m[0011] [reconcile] Rebuilding and updating local kube config \r\n\u001b[36mINFO\u001b[39m[0011] Successfully Deployed local admin kubeconfig at [./kube_config_cluster.yml] \u001b[1;33r\u001b[32;1H"]
[332.924659, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0011] [reconcile] host [10.3.3.151] is a control plane node with reachable Kubernetes API endpoint in the cluster \r\n\u001b[36mINFO\u001b[39m[0011] [reconcile] Reconciled cluster state successfully \r\n\u001b[36mINFO\u001b[39m[0011] max_unavailable_worker got rounded down to 0, resetting to 1 \u001b[1;33r\u001b[32;1H\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0011] Setting maxUnavailable for worker nodes to: 1 \r\n\u001b[36mINFO\u001b[39m[0011] Setting maxUnavailable for controlplane nodes to: 1 "]
[332.924752, "o", "\u001b[1;33r\u001b[32;1H"]
[332.926131, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0011] Restarting container [kube-apiserver] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[338.111216, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0016] [restart/kube-apiserver] Successfully restarted container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0016] Pre-pulling kubernetes images                \u001b[1;33r\u001b[32;1H"]
[338.113821, "o", "\u001b[1;32r\u001b[4S\u001b[28d\u001b[36mINFO\u001b[39m[0016] Image [rancher/hyperkube:v1.27.8-rancher2] exists on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0016] Image [rancher/hyperkube:v1.27.8-rancher2] exists on host [10.3.1.253] \r\n\u001b[36mINFO\u001b[39m[0016] Image [rancher/hyperkube:v1.27.8-rancher2] exists on host [10.3.1.91] \r\n\u001b[36mINFO\u001b[39m[0016] Image [rancher/hyperkube:v1.27.8-rancher2] exists on host [10.3.3.13] \u001b[1;33r\u001b[32;1H"]
[338.114483, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0016] Image [rancher/mirrored-pause:3.7] exists on host [10.3.1.253] \u001b[1;33r\u001b[32;1H"]
[338.114757, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0016] Image [rancher/mirrored-pause:3.7] exists on host [10.3.3.13] \r\n\u001b[36mINFO\u001b[39m[0016] Image [rancher/mirrored-pause:3.7] exists on host [10.3.1.91] \u001b[1;33r\u001b[32;1H"]
[338.116842, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0016] Image [rancher/mirrored-pause:3.7] exists on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0016] Kubernetes images pulled successfully        \u001b[1;33r\u001b[32;1H\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0016] [etcd] Building up etcd plane..              \u001b[1;33r\u001b[32;1H"]
[338.119204, "o", "\u001b[36mINFO\u001b[39m[0016] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;32r\u001b[32;1H\n\u001b[1;33r\u001b[32;1H"]
[338.978337, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0017] Starting container [etcd-fix-perm] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[339.243542, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0017] Successfully started [etcd-fix-perm] container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0017] Waiting for [etcd-fix-perm] container to exit on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0017] Waiting for [etcd-fix-perm] container to exit on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[339.247326, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0017] Container [etcd-fix-perm] is still running on host [10.3.3.151]: stderr: [], stdout: [] \u001b[1;33r\u001b[32;1H"]
[340.250459, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0018] Removing container [etcd-fix-perm] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[340.269211, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0018] [remove/etcd-fix-perm] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[340.27222, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0018] [etcd] Running rolling snapshot container [etcd-rolling-snapshots] on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[340.273139, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0018] Removing container [etcd-rolling-snapshots] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[340.357722, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0018] [remove/etcd-rolling-snapshots] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[340.360198, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0018] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[340.939481, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0019] Starting container [etcd-rolling-snapshots] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[341.081091, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0019] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[346.089222, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0024] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[346.665006, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0024] Starting container [rke-bundle-cert] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[346.87865, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0025] [certificates] Successfully started [rke-bundle-cert] container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0025] Waiting for [rke-bundle-cert] container to exit on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[346.880737, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0025] Container [rke-bundle-cert] is still running on host [10.3.3.151]: stderr: [], stdout: [] \u001b[1;33r\u001b[32;1H"]
[347.882771, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0026] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0026] Removing container [rke-bundle-cert] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[347.9055, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0026] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[348.512836, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0026] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[348.75984, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0027] [etcd] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[348.761124, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0027] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[348.924846, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0027] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[348.928823, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0027] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[349.551059, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0027] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[349.76685, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0028] [etcd] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[349.768337, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0028] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[349.952196, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0028] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0028] [etcd] Successfully started etcd plane.. Checking etcd cluster health \u001b[1;33r\u001b[32;1H"]
[350.438396, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0028] [etcd] etcd host [10.3.3.151] reported healthy=true \u001b[1;33r\u001b[32;1H"]
[350.43903, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0028] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[1;33r\u001b[32;1H"]
[375.476041, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0053] Attempting upgrade of controlplane components on following hosts in NotReady status: 10.3.3.151 \r\n\u001b[36mINFO\u001b[39m[0053] [controlplane] Building up Controller Plane.. \u001b[1;33r\u001b[32;1H\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0053] Finding container [service-sidekick] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[375.483085, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0053] [sidekick] Sidekick container already created on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[375.487805, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0053] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[376.040491, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0054] [healthcheck] service [kube-apiserver] on host [10.3.3.151] is healthy \u001b[1;33r\u001b[32;1H"]
[376.044768, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0054] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[376.525506, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0054] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[376.73963, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0055] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[376.741037, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0055] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[376.916136, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0055] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[376.919611, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0055] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[377.44054, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0055] [healthcheck] service [kube-controller-manager] on host [10.3.3.151] is healthy \u001b[1;33r\u001b[32;1H"]
[377.444294, "o", "\u001b[36mINFO\u001b[39m[0055] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;32r\u001b[32;1H\n\u001b[1;33r\u001b[32;1H"]
[377.927932, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0056] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[378.132843, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0056] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[378.134198, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0056] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[378.287698, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0056] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[378.291004, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0056] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[378.776673, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0057] [healthcheck] service [kube-scheduler] on host [10.3.3.151] is healthy \u001b[1;33r\u001b[32;1H"]
[378.779944, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0057] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[379.255415, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0057] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[379.456988, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0057] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[379.458248, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0057] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[379.620595, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0057] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0057] [controlplane] Successfully started Controller Plane.. \r\n\u001b[36mINFO\u001b[39m[0057] [worker] Building up Worker Plane..          \u001b[1;33r\u001b[32;1H\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0057] Finding container [service-sidekick] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[379.626904, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0057] [sidekick] Sidekick container already created on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[379.631767, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0057] [healthcheck] Start Healthcheck on service [kubelet] on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[391.852087, "o", "\u001b[?25l\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud*Z 2:AIDA-Edge-                                                                \"masterk8s\" 18:02 05-Feb-24\u001b[m\u000f\u001b[32;1H\u001b[34h\u001b[?25h"]
[435.282464, "o", "\u001b[1;32r\u001b[5S\u001b[27d\u001b[31mERRO\u001b[39m[0113] Failed to upgrade worker components on NotReady hosts, error: [Failed to verify healthcheck: Failed to check http://localhost:10248/healthz for service [kubelet] on host [10.3.3.151]: Get \"http://localhost:10248/healthz\": Unable to access the service on localhost:10248. The service might be still starting up. Error: ssh: rejected: connect failed (Connection refused), log: time=\"2024-02-05T18:02:52Z\" level=info msg=\"Start cri-dockerd grpc backend\"] \r\n\u001b[36mINFO\u001b[39m[0113] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[1;33r\u001b[32;1H"]
[451.874851, "o", "\u001b[?25l\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud*Z 2:AIDA-Edge-                                                                \"masterk8s\" 18:03 05-Feb-24\u001b[m\u000f\u001b[32;1H\u001b[34h\u001b[?25h"]
[456.348383, "o", "\u001b[?25l\u001b[36m\u001b[HINFO\u001b[39m[0053] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.3.3.151]                           \u001b[30m\u001b[43m[0/762]\u001b[49m\u001b[36m\u001b[2;1HINFO\u001b[39m[0054] [healthcheck] service [kube-apiserver] on host [10.3.3.151] is healthy \u001b[K\u001b[36m\r\nINFO\u001b[39m[0054] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0054] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0055] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0055] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0055] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0055] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0055] [healthcheck] service [kube-controller-manager] on host [10.3.3.151] is healthy \u001b[K\u001b[36m\r\nINFO\u001b[39m[0055] Image [rancher/rke-tools:v0.1.96] exi"]
[456.348451, "o", "sts on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [healthcheck] service [kube-scheduler] on host [10.3.3.151] is healthy \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0"]
[456.348479, "o", "057] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [controlplane] Successfully started Controller Plane.. \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [worker] Building up Worker Plane..          \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] Finding container [service-sidekick] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [sidekick] Sidekick container already created on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [healthcheck] Start Healthcheck on service [kubelet] on host [10.3.3.151] \u001b[K\u001b[30m\u001b[43m\r\nER\u001b[49m\u001b[31mRO\u001b[39m[0113] Failed to upgrade worker components on NotReady hosts, error: [Failed to verify healthcheck: Failed to check http:\u001b[28;1H//localhost:10248/healthz for service [kubelet] on host [10.3.3.151]: Get \"http://localhost:10248/healthz\": Unable to access \u001b[29;1Hthe service on localhost:10248. The service might be still starting up. Error: ssh: rejected: connect failed (Connection refu\u001b[30;1Hsed), log: time=\"2024-02-05T18:02:52Z\" level=info msg=\"Start cri-dockerd grpc ba"]
[456.348499, "o", "ckend\"] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0113] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[K\r\n\u001b[K\u001b[27;2H\u001b[34h\u001b[?25h"]
[456.419815, "o", "\r\u001b[30m\u001b[43mERR\rERRO\rERRO[\rERRO[0\rERRO[01\b\u001b[m\u000f"]
[456.43467, "o", "\r\u001b[30m\u001b[43mERRO[011\b\u001b[m\u000f"]
[456.488553, "o", "\r\u001b[30m\u001b[43mERRO[0113\rERRO[0113]\rERRO[0113] \rERRO[0113] F\b\u001b[m\u000f"]
[456.557314, "o", "\r\u001b[30m\u001b[43mERRO[0113] Fa\rERRO[0113] Fai\b\u001b[m\u000f"]
[456.572199, "o", "\r\u001b[30m\u001b[43mERRO[0113] Fail\b\u001b[m\u000f"]
[456.599358, "o", "\r\u001b[30m\u001b[43mERRO[0113] Faile\b\u001b[m\u000f"]
[456.626597, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed\rERRO[0113] Failed \b\u001b[m\u000f"]
[456.641543, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed t\b\u001b[m\u000f"]
[456.678506, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to\b\u001b[m\u000f"]
[456.698629, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to \b\u001b[m\u000f"]
[456.810907, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to u\b\u001b[m\u000f"]
[456.881799, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to up\rERRO[0113] Failed to upg\rERRO[0113] Failed to upgr\rERRO[0113] Failed to upgra\b\u001b[m\u000f"]
[456.896429, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrad\rERRO[0113] Failed to upgrade\rERRO[0113] Failed to upgrade \b\u001b[m\u000f"]
[456.950835, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade w\rERRO[0113] Failed to upgrade wo\rERRO[0113] Failed to upgrade wor\rERRO[0113] Failed to upgrade work\rERRO[0113] Failed to upgrade worke\b\u001b[m\u000f"]
[456.96573, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker\rERRO[0113] Failed to upgrade worker \rERRO[0113] Failed to upgrade worker co\b\u001b[m\u000f"]
[457.019824, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker com\rERRO[0113] Failed to upgrade worker compo\rERRO[0113] Failed to upgrade worker compon\rERRO[0113] Failed to upgrade worker componen\rERRO[0113] Failed to upgrade worker component\rERRO[0113] Failed to upgrade worker components\rERRO[0113] Failed to upgrade worker components o\b\u001b[m\u000f"]
[457.034437, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on\b\u001b[m\u000f"]
[457.089371, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on \rERRO[0113] Failed to upgrade worker components on N\rERRO[0113] Failed to upgrade worker components on No\b\u001b[m\u000f"]
[457.10469, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on Not\b\u001b[m\u000f"]
[457.158566, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotR\b\u001b[m\u000f"]
[457.17356, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotRe\b\u001b[m\u000f"]
[457.227968, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotRea\b\u001b[m\u000f"]
[457.242218, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotRead\b\u001b[m\u000f"]
[457.296869, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotReady\b\u001b[m\u000f"]
[457.377872, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotReady \rERRO[0113] Failed to upgrade worker components on NotReady h\b\u001b[m\u000f"]
[457.392446, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotReady ho\b\u001b[m\u000f"]
[457.447619, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotReady hos\b\u001b[m\u000f"]
[457.516611, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotReady host\rERRO[0113] Failed to upgrade worker components on NotReady hosts\b\u001b[m\u000f"]
[457.549191, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotReady hosts,\b\u001b[m\u000f"]
[457.568165, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotReady hosts, \b\u001b[m\u000f"]
[457.588287, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotReady hosts, e\b\u001b[m\u000f"]
[457.628266, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotReady hosts, er\b\u001b[m\u000f"]
[457.668744, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotReady hosts, err\b\u001b[m\u000f"]
[457.781731, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotReady hosts, erro\b\u001b[m\u000f"]
[458.396716, "o", "\r\u001b[30m\u001b[43mERRO[0113] Failed to upgrade worker components on NotReady hosts, error\b\u001b[m\u000f"]
[458.888215, "o", "\u001b[?25l\u001b[36m\u001b[HINFO\u001b[39m[0053] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0054] [healthcheck] service [kube-apiserver] on host [10.3.3.151] is healthy \u001b[K\u001b[36m\r\nINFO\u001b[39m[0054] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0054] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0055] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0055] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0055] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0055] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0055] [healthcheck] service [kube-controller-manager] on host [10.3.3.151] is healthy \u001b[K\u001b[36m\r\nINFO\u001b[39m[0055] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056"]
[458.888259, "o", "] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [healthcheck] service [kube-scheduler] on host [10.3.3.151] is healthy \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [remove/rke-log-linker] Successfully removed"]
[458.888279, "o", " container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [controlplane] Successfully started Controller Plane.. \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [worker] Building up Worker Plane..          \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] Finding container [service-sidekick] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [sidekick] Sidekick container already created on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [healthcheck] Start Healthcheck on service [kubelet] on host [10.3.3.151] \u001b[K\u001b[31m\r\nERRO\u001b[39m[0113] Failed to upgrade worker components on NotReady hosts, error: [Failed to verify healthcheck: Failed to check http://localhost:10248/healthz for service [kubelet] on host [10.3.3.151]: Get \"http://localhost:10248/healthz\": Unable to access the service on localhost:10248. The service might be still starting up. Error: ssh: rejected: connect failed (Connection refused), log: time=\"2024-02-05T18:02:52Z\" level=info msg=\"Start cri-dockerd grpc backend\"] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0113] [controlplane] Now checking status of node 10.3.3.1"]
[458.888302, "o", "51, try #1 \u001b[K\r\n\u001b[K\u001b[34h\u001b[?25h"]
[460.304405, "o", "\u001b[1;32r\u001b[5S\u001b[27d\u001b[31mERRO\u001b[39m[0138] Host 10.3.3.151 failed to report Ready status with error: [controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found \r\n\u001b[36mINFO\u001b[39m[0138] [controlplane] Processing controlplane hosts for upgrade 1 at a time \r\n\u001b[36mINFO\u001b[39m[0138] Processing controlplane host 10.3.3.151      \r\n\u001b[36mINFO\u001b[39m[0138] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[1;33r\u001b[32;1H"]
[485.331851, "o", "\u001b[1;32r\u001b[4S\u001b[28d\u001b[31mERRO\u001b[39m[0163] Failed to upgrade hosts: 10.3.3.151 with error [[controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found] \r\n\u001b[31mFATA\u001b[39m[0163] [controlPlane] Failed to upgrade Control Plane: [[[controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found]] \u001b[1;33r\u001b[32;1H"]
[485.351811, "o", "\u001b[32m\u001b[1maida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ "]
[490.628565, "o", "\u001b[1;33r\u001b[m\u000f\u001b[?1l\u001b>\u001b[H\u001b[J\u001b[34h\u001b[?25h\u001b[?1000l\u001b[?1002l\u001b[?1006l\u001b[?1005l\u001b[?1049l"]
[490.628698, "o", "[detached (from session AIDA)]\r\n"]
[490.638167, "o", "\u001b[?1049h\u001b[?1h\u001b=\u001b[H\u001b[J\u001b[34h\u001b[?25h\u001b[?1000l\u001b[?1002l\u001b[?1006l\u001b[?1005l\u001b[c\u001b[m\u000f\u001b[34l\u001b[34h\u001b[?25h\u001b[?1003l\u001b[?1006l\u001b[?2004l\u001b[1;1H\u001b[1;33r\u001b[32;19H\u001b[?1006h\u001b[?1002h\u001b[?25l\u001b[36m\u001b[HINFO\u001b[39m[0055] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [healthcheck] service [kube-scheduler] on host [10.3.3.151] is healthy \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36"]
[490.639607, "o", "m\r\nINFO\u001b[39m[0058] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] [controlplane] Successfully started Controller Plane.. \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] [worker] Building up Worker Plane..          \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] Finding container [service-sidekick] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] [sidekick] Sidekick container already created on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] [healthcheck] Start Healthcheck on service [kubelet] on host [10.3.3.151] \u001b[K\u001b[31m\r\nERRO\u001b[39m[0113] Failed to upgrade worker components on NotReady hosts, error: [Failed to verify healthcheck: Failed to check http://localhost:10248/healthz for service [kubelet] on host [10.3.3.151]: Get \"http://localhost:10248/healthz\": Unable to access the service on localhost:1"]
[490.639681, "o", "0248. The service might be still starting up. Error: ssh: rejected: connect failed (Connection refused), log: time=\"2024-02-05T18:08:04Z\" level=info msg=\"Start cri-dockerd grpc backend\"] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0113] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[K\u001b[31m\r\nERRO\u001b[39m[0138] Host 10.3.3.151 failed to report Ready status with error: [controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found \u001b[K\u001b[36m\r\nINFO\u001b[39m[0138] [controlplane] Processing controlplane hosts for upgrade 1 at a time \u001b[K\u001b[36m\r\nINFO\u001b[39m[0138] Processing controlplane host 10.3.3.151      \u001b[K\u001b[36m\r\nINFO\u001b[39m[0138] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[K\u001b[31m\r\nERRO\u001b[39m[0163] Failed to upgrade hosts: 10.3.3.151 with error [[controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found] \u001b[K\u001b[31m\r\nFATA\u001b[39m[0163] [controlPlane] Failed to upgrade Control Plane: [[[controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found]] \u001b[K\u001b[32m\u001b[1m\r\naida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$"]
[490.639744, "o", " \u001b[K\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud*Z 2:AIDA-Edge-                                                                \"masterk8s\" 18:35 05-Feb-24\u001b[m\u000f\u001b[32;19H\u001b[34h\u001b[?25h\u001b[m\u000f\u001b[34l\u001b[34h\u001b[?25h\u001b[?1003l\u001b[?1006l\u001b[?2004l\u001b[1;1H\u001b[1;33r\u001b[32;19H\u001b[?1006h\u001b[?1002h\u001b[?25l\u001b[36m\u001b[HINFO\u001b[39m[0055] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0056] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] [healthcheck] service [kube-scheduler] on host [10.3.3.151] is healthy \u001b[K\u001b[36m\r\nINFO\u001b[39m[0057] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b"]
[490.639805, "o", "[K\u001b[36m\r\nINFO\u001b[39m[0058] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] [controlplane] Successfully started Controller Plane.. \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] [worker] Building up Worker Plane..          \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] Finding container [service-sidekick] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] [sidekick] Sidekick container already created on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0058] [healthcheck] Start Healthcheck on service [kubelet] on host [10.3.3.151] \u001b[K\u001b[31m\r\nERRO\u001b[39m[0113] Failed to upgrade worker components on NotReady hosts, error: [Failed to verify healthcheck: Failed to check http://localhost:10248/healthz for service [kubelet] on hos"]
[490.639857, "o", "t [10.3.3.151]: Get \"http://localhost:10248/healthz\": Unable to access the service on localhost:10248. The service might be still starting up. Error: ssh: rejected: connect failed (Connection refused), log: time=\"2024-02-05T18:08:04Z\" level=info msg=\"Start cri-dockerd grpc backend\"] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0113] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[K\u001b[31m\r\nERRO\u001b[39m[0138] Host 10.3.3.151 failed to report Ready status with error: [controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found \u001b[K\u001b[36m\r\nINFO\u001b[39m[0138] [controlplane] Processing controlplane hosts for upgrade 1 at a time \u001b[K\u001b[36m\r\nINFO\u001b[39m[0138] Processing controlplane host 10.3.3.151      \u001b[K\u001b[36m\r\nINFO\u001b[39m[0138] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[K\u001b[31m\r\nERRO\u001b[39m[0163] Failed to upgrade hosts: 10.3.3.151 with error [[controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found] \u001b[K\u001b[31m\r\nFATA\u001b[39m[0163] [controlPlane] Failed to upgrade Control Plane: [[[controlplane] Error ge"]
[490.639958, "o", "tting node 10.3.3.151:  \"10.3.3.151\" not found]] \u001b[K\u001b[32m\u001b[1m\r\naida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud*Z 2:AIDA-Edge-                                                                \"masterk8s\" 18:35 05-Feb-24\u001b[m\u000f\u001b[32;19H\u001b[34h\u001b[?25h"]
[491.420382, "o", "\u001b[1;32r\u001b[2;32r\u001b[31S\u001b[1;1H\u001b[K\u001b[32m\u001b[1maida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[1;33r\u001b[1;19H"]
[491.971226, "o", "rke up --ignore-docker-version"]
[494.209303, "o", "\r\n"]
[495.208851, "o", "\u001b[36mINFO\u001b[39m[0000] Running RKE version: v1.5.3                  \r\n"]
[495.223188, "o", "\u001b[36mINFO\u001b[39m[0000] Initiating Kubernetes cluster                \r\n"]
[495.416294, "o", "\u001b[36mINFO\u001b[39m[0000] [certificates] GenerateServingCertificate is disabled, checking if there are unused kubelet certificates \r\n\u001b[36mINFO\u001b[39m[0000] [certificates] Generating admin certificates and kubeconfig \r\n"]
[495.421201, "o", "\u001b[36mINFO\u001b[39m[0000] Successfully Deployed state file at [./cluster.rkestate] \r\n"]
[495.430496, "o", "\u001b[36mINFO\u001b[39m[0000] Building Kubernetes cluster                  \r\n"]
[495.432699, "o", "\u001b[36mINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.1.253]  \r\n"]
[495.432999, "o", "\u001b[36mINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.3.13]   \r\n\u001b[36mINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.1.91]   \r\n\u001b[36mINFO\u001b[39m[0000] [dialer] Setup tunnel for host [10.3.3.151]  \r\n"]
[496.761352, "o", "\u001b[36mINFO\u001b[39m[0001] [network] No hosts added existing cluster, skipping port check \r\n\u001b[36mINFO\u001b[39m[0001] [certificates] Deploying kubernetes certificates to Cluster nodes \r\n\u001b[36mINFO\u001b[39m[0001] Finding container [cert-deployer] on host [10.3.1.253], try #1 \r\n\u001b[36mINFO\u001b[39m[0001] Finding container [cert-deployer] on host [10.3.1.91], try #1 \r\n\u001b[36mINFO\u001b[39m[0001] Finding container [cert-deployer] on host [10.3.3.13], try #1 \r\n\u001b[36mINFO\u001b[39m[0001] Finding container [cert-deployer] on host [10.3.3.151], try #1 \r\n"]
[496.763509, "o", "\u001b[36mINFO\u001b[39m[0001] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.1.91] \r\n\u001b[36mINFO\u001b[39m[0001] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.1.253] \r\n\u001b[36mINFO\u001b[39m[0001] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.13] \r\n"]
[496.803941, "o", "\u001b[36mINFO\u001b[39m[0001] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \r\n"]
[497.804025, "o", "\u001b[36mINFO\u001b[39m[0002] Starting container [cert-deployer] on host [10.3.1.253], try #1 \r\n"]
[497.866584, "o", "\u001b[36mINFO\u001b[39m[0003] Starting container [cert-deployer] on host [10.3.3.13], try #1 \r\n"]
[498.274046, "o", "\u001b[36mINFO\u001b[39m[0003] Finding container [cert-deployer] on host [10.3.1.253], try #1 \r\n"]
[498.289428, "o", "\u001b[36mINFO\u001b[39m[0003] Starting container [cert-deployer] on host [10.3.1.91], try #1 \r\n"]
[498.521142, "o", "\u001b[36mINFO\u001b[39m[0003] Finding container [cert-deployer] on host [10.3.3.13], try #1 \r\n"]
[498.83945, "o", "\u001b[36mINFO\u001b[39m[0003] Finding container [cert-deployer] on host [10.3.1.91], try #1 \r\n"]
[498.849037, "o", "\u001b[36mINFO\u001b[39m[0003] Starting container [cert-deployer] on host [10.3.3.151], try #1 \r\n"]
[499.210479, "o", "\u001b[36mINFO\u001b[39m[0004] Finding container [cert-deployer] on host [10.3.3.151], try #1 \r\n"]
[503.283715, "o", "\u001b[36mINFO\u001b[39m[0008] Finding container [cert-deployer] on host [10.3.1.253], try #1 \r\n"]
[503.287026, "o", "\u001b[36mINFO\u001b[39m[0008] Removing container [cert-deployer] on host [10.3.1.253], try #1 \r\n"]
[503.527051, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0008] Finding container [cert-deployer] on host [10.3.3.13], try #1 \u001b[1;33r\u001b[32;1H"]
[503.530143, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0008] Removing container [cert-deployer] on host [10.3.3.13], try #1 \u001b[1;33r\u001b[32;1H"]
[503.843183, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0008] Finding container [cert-deployer] on host [10.3.1.91], try #1 \u001b[1;33r\u001b[32;1H"]
[503.845631, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0008] Removing container [cert-deployer] on host [10.3.1.91], try #1 \u001b[1;33r\u001b[32;1H"]
[504.215274, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0009] Finding container [cert-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[504.220148, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0009] Removing container [cert-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[504.235767, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0009] [reconcile] Rebuilding and updating local kube config \u001b[1;33r\u001b[32;1H"]
[504.236235, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0009] Successfully Deployed local admin kubeconfig at [./kube_config_cluster.yml] \u001b[1;33r\u001b[32;1H"]
[504.271834, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0009] [reconcile] host [10.3.3.151] is a control plane node with reachable Kubernetes API endpoint in the cluster \r\n\u001b[36mINFO\u001b[39m[0009] [certificates] Successfully deployed kubernetes certificates to Cluster nodes \u001b[1;33r\u001b[32;1H"]
[504.272462, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0009] [file-deploy] Deploying file [/etc/kubernetes/admission.yaml] to node [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[504.282242, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0009] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[508.110901, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0013] Starting container [file-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[508.781694, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0013] Successfully started [file-deployer] container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0013] Waiting for [file-deployer] container to exit on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0013] Waiting for [file-deployer] container to exit on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[508.786432, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0013] Container [file-deployer] is still running on host [10.3.3.151]: stderr: [], stdout: [] \u001b[1;33r\u001b[32;1H"]
[509.789572, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0014] Removing container [file-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[509.822526, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0014] [remove/file-deployer] Successfully removed container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0014] [/etc/kubernetes/admission.yaml] Successfully deployed admission control config to Cluster control nodes \u001b[1;33r\u001b[32;1H"]
[509.826323, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0014] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[509.830444, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0014] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[510.490845, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0015] Starting container [file-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[510.843372, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0015] Successfully started [file-deployer] container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0015] Waiting for [file-deployer] container to exit on host [10.3.3.151] \u001b[1;33r\u001b[32;1H\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0015] Waiting for [file-deployer] container to exit on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[510.844763, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0015] Container [file-deployer] is still running on host [10.3.3.151]: stderr: [], stdout: [] \u001b[1;33r\u001b[32;1H"]
[511.848511, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0016] Removing container [file-deployer] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[511.878113, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0017] [remove/file-deployer] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[511.878662, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0017] [/etc/kubernetes/audit-policy.yaml] Successfully deployed audit policy file to Cluster control nodes \u001b[1;33r\u001b[32;1H"]
[511.880243, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0017] [reconcile] Reconciling cluster state        \u001b[1;33r\u001b[32;1H"]
[511.880652, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0017] [reconcile] Check etcd hosts to be deleted   \u001b[1;33r\u001b[32;1H"]
[511.88525, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0017] [reconcile] Check etcd hosts to be added     \r\n\u001b[36mINFO\u001b[39m[0017] [reconcile] Rebuilding and updating local kube config \r\n\u001b[36mINFO\u001b[39m[0017] Successfully Deployed local admin kubeconfig at [./kube_config_cluster.yml] \u001b[1;33r\u001b[32;1H"]
[511.888062, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0017] [reconcile] host [10.3.3.151] is a control plane node with reachable Kubernetes API endpoint in the cluster \r\n\u001b[36mINFO\u001b[39m[0017] [reconcile] Reconciled cluster state successfully \u001b[1;33r\u001b[32;1H"]
[511.888566, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0017] max_unavailable_worker got rounded down to 0, resetting to 1 \r\n\u001b[36mINFO\u001b[39m[0017] Setting maxUnavailable for worker nodes to: 1 \r\n\u001b[36mINFO\u001b[39m[0017] Setting maxUnavailable for controlplane nodes to: 1 \u001b[1;33r\u001b[32;1H"]
[511.891223, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0017] Restarting container [kube-apiserver] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[517.168334, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0022] [restart/kube-apiserver] Successfully restarted container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0022] Pre-pulling kubernetes images                \u001b[1;33r\u001b[32;1H"]
[517.181615, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0022] Image [rancher/hyperkube:v1.27.8-rancher2] exists on host [10.3.1.253] \r\n\u001b[36mINFO\u001b[39m[0022] Image [rancher/hyperkube:v1.27.8-rancher2] exists on host [10.3.1.91] \r\n\u001b[36mINFO\u001b[39m[0022] Image [rancher/hyperkube:v1.27.8-rancher2] exists on host [10.3.3.13] \u001b[1;33r\u001b[32;1H"]
[517.182991, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0022] Image [rancher/mirrored-pause:3.7] exists on host [10.3.1.253] \u001b[1;33r\u001b[32;1H"]
[517.183172, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0022] Image [rancher/mirrored-pause:3.7] exists on host [10.3.3.13] \r\n\u001b[36mINFO\u001b[39m[0022] Image [rancher/mirrored-pause:3.7] exists on host [10.3.1.91] \u001b[1;33r\u001b[32;1H"]
[517.185473, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0022] Image [rancher/hyperkube:v1.27.8-rancher2] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[517.189989, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0022] Image [rancher/mirrored-pause:3.7] exists on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0022] Kubernetes images pulled successfully        \u001b[1;33r\u001b[32;1H"]
[517.192997, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0022] [etcd] Building up etcd plane..              \u001b[1;33r\u001b[32;1H"]
[517.195695, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0022] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[517.815377, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0022] Starting container [etcd-fix-perm] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[518.13699, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0023] Successfully started [etcd-fix-perm] container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0023] Waiting for [etcd-fix-perm] container to exit on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0023] Waiting for [etcd-fix-perm] container to exit on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[518.138914, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0023] Container [etcd-fix-perm] is still running on host [10.3.3.151]: stderr: [], stdout: [] \u001b[1;33r\u001b[32;1H"]
[519.142215, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0024] Removing container [etcd-fix-perm] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[519.162916, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0024] [remove/etcd-fix-perm] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[519.171131, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0024] [etcd] Running rolling snapshot container [etcd-rolling-snapshots] on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[519.172782, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0024] Removing container [etcd-rolling-snapshots] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[519.290018, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0024] [remove/etcd-rolling-snapshots] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[519.292575, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0024] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[519.902601, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0025] Starting container [etcd-rolling-snapshots] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[520.10042, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0025] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[525.110086, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0030] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[525.648711, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0030] Starting container [rke-bundle-cert] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[525.918428, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0031] [certificates] Successfully started [rke-bundle-cert] container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0031] Waiting for [rke-bundle-cert] container to exit on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[525.920956, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0031] Container [rke-bundle-cert] is still running on host [10.3.3.151]: stderr: [], stdout: [] \u001b[1;33r\u001b[32;1H"]
[526.923249, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[36mINFO\u001b[39m[0032] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0032] Removing container [rke-bundle-cert] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[526.950462, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0032] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[527.482731, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0032] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[527.719119, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0032] [etcd] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[527.720462, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0032] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[527.941591, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0033] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[527.945519, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0033] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[528.48505, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0033] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[528.75178, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0033] [etcd] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[528.752791, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0033] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[528.939607, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0034] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0034] [etcd] Successfully started etcd plane.. Checking etcd cluster health \u001b[1;33r\u001b[32;1H"]
[529.310066, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0034] [etcd] etcd host [10.3.3.151] reported healthy=true \u001b[1;33r\u001b[32;1H"]
[529.310468, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0034] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[1;33r\u001b[32;1H"]
[550.658353, "o", "\u001b[?25l\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud*Z 2:AIDA-Edge-                                                                \"masterk8s\" 18:36 05-Feb-24\u001b[m\u000f\u001b[32;1H\u001b[34h\u001b[?25h"]
[554.369284, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0059] Attempting upgrade of controlplane components on following hosts in NotReady status: 10.3.3.151 \r\n\u001b[36mINFO\u001b[39m[0059] [controlplane] Building up Controller Plane.. \u001b[1;33r\u001b[32;1H"]
[554.388361, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0059] Finding container [service-sidekick] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[554.550791, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0059] [sidekick] Sidekick container already created on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[554.559721, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0059] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[555.055226, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0060] [healthcheck] service [kube-apiserver] on host [10.3.3.151] is healthy \u001b[1;33r\u001b[32;1H"]
[555.067722, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0060] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[557.374209, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0062] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[557.858503, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0062] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[557.859967, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0063] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[558.046882, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0063] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[558.052776, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0063] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[558.323901, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0063] [healthcheck] service [kube-controller-manager] on host [10.3.3.151] is healthy \u001b[1;33r\u001b[32;1H"]
[558.328347, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0063] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[558.898484, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0064] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[559.146745, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0064] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[559.148302, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0064] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[559.373041, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0064] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[559.376816, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0064] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[559.636643, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0064] [healthcheck] service [kube-scheduler] on host [10.3.3.151] is healthy \u001b[1;33r\u001b[32;1H"]
[559.641309, "o", "\u001b[36mINFO\u001b[39m[0064] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[1;32r\u001b[32;1H\n\u001b[1;33r\u001b[32;1H"]
[560.215053, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0065] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[560.489089, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0065] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[560.490265, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0065] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[560.667952, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0065] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \r\n\u001b[36mINFO\u001b[39m[0065] [controlplane] Successfully started Controller Plane.. \u001b[1;33r\u001b[32;1H"]
[560.668548, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0065] [worker] Building up Worker Plane..          \u001b[1;33r\u001b[32;1H"]
[560.669745, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0065] Finding container [service-sidekick] on host [10.3.3.151], try #1 \u001b[1;33r\u001b[32;1H"]
[560.680891, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0065] [sidekick] Sidekick container already created on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[560.684591, "o", "\u001b[1;32r\u001b[32;1H\n\u001bM\u001b[36mINFO\u001b[39m[0065] [healthcheck] Start Healthcheck on service [kubelet] on host [10.3.3.151] \u001b[1;33r\u001b[32;1H"]
[579.422376, "o", "\u001b[32m\u001b[11d─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[22;1H─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[?25l\u001b[m\u000f\u001b[1;1H\u001b[K\r\n\u001b[K\r\n*** System restart required ***\u001b[K\r\nLast login: Fri Jan 26 09:52:14 2024 from 10.3.3.151\u001b[K\u001b[32m\u001b[1m\r\nadmin@attacker\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ ip -brief add\u001b[K\r\nlo               UNKNOWN        127.0.0.1/8 ::1/128 \u001b[K\r\neth0             UP"]
[579.422467, "o", "             10.3.3.202/16 fe80::4406:6aff:fed6:5f4e/64 \u001b[K\r\nlxcbr0           DOWN           10.0.3.1/24 \u001b[K\u001b[32m\u001b[1m\r\nadmin@attacker\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ source .profile \u001b[K\u001b[32m\u001b[1m\r\nadmin@attacker\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\u001b[32m\u001b[1m\u001b[12;1Haida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\u001b[32m\u001b[1m\u001b[2Baida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ \u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud-Z 2:AIDA-Edge*                                                                \"masterk8s\" 18:36 05-Feb-24\u001b[m\u000f\u001b[12;19H\u001b[34h\u001b[?25h"]
[580.386857, "o", "\u001b[?25l\u001b[36m\u001b[HINFO\u001b[39m[0034] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0059] Attempting upgrade of controlplane components on following hosts in NotReady status: 10.3.3.151 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0059] [controlplane] Building up Controller Plane.. \u001b[K\u001b[36m\r\nINFO\u001b[39m[0059] Finding container [service-sidekick] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0059] [sidekick] Sidekick container already created on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0059] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0060] [healthcheck] service [kube-apiserver] on host [10.3.3.151] is healthy \u001b[K\u001b[36m\r\nINFO\u001b[39m[0060] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0062] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0062] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0063] Removing container [rke-log-linker"]
[580.386931, "o", "] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0063] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0063] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0063] [healthcheck] service [kube-controller-manager] on host [10.3.3.151] is healthy \u001b[K\u001b[36m\r\nINFO\u001b[39m[0063] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0064] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0064] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0064] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0064] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0064] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0064] [healthcheck] service [kube-scheduler] on host [10.3.3"]
[580.386941, "o", ".151] is healthy \u001b[K\u001b[36m\r\nINFO\u001b[39m[0064] Image [rancher/rke-tools:v0.1.96] exists on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0065] Starting container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0065] [controlplane] Successfully started [rke-log-linker] container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0065] Removing container [rke-log-linker] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0065] [remove/rke-log-linker] Successfully removed container on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0065] [controlplane] Successfully started Controller Plane.. \u001b[K\u001b[36m\r\nINFO\u001b[39m[0065] [worker] Building up Worker Plane..          \u001b[K\u001b[36m\r\nINFO\u001b[39m[0065] Finding container [service-sidekick] on host [10.3.3.151], try #1 \u001b[K\u001b[36m\r\nINFO\u001b[39m[0065] [sidekick] Sidekick container already created on host [10.3.3.151] \u001b[K\u001b[36m\r\nINFO\u001b[39m[0065] [healthcheck] Start Healthcheck on service [kubelet] on host [10.3.3.151] \u001b[K\r\n\u001b[K\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud*Z 2:AIDA-Edge-                                  "]
[580.386952, "o", "                              \"masterk8s\" 18:36 05-Feb-24\u001b[m\u000f\u001b[32;1H\u001b[34h\u001b[?25h"]
[610.67897, "o", "\u001b[?25l\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud*Z 2:AIDA-Edge-                                                                \"masterk8s\" 18:37 05-Feb-24\u001b[m\u000f\u001b[32;1H\u001b[34h\u001b[?25h"]
[616.156055, "o", "\u001b[1;32r\u001b[5S\u001b[27d\u001b[31mERRO\u001b[39m[0121] Failed to upgrade worker components on NotReady hosts, error: [Failed to verify healthcheck: Failed to check http://localhost:10248/healthz for service [kubelet] on host [10.3.3.151]: Get \"http://localhost:10248/healthz\": Unable to access the service on localhost:10248. The service might be still starting up. Error: ssh: rejected: connect failed (Connection refused), log: time=\"2024-02-05T18:37:13Z\" level=info msg=\"pid 2964057 is not running in the host namespaces\"] \r\n\u001b[36mINFO\u001b[39m[0121] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[1;33r\u001b[32;1H"]
[641.190828, "o", "\u001b[1;32r\u001b[3S\u001b[29d\u001b[31mERRO\u001b[39m[0146] Host 10.3.3.151 failed to report Ready status with error: [controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found \r\n\u001b[36mINFO\u001b[39m[0146] [controlplane] Processing controlplane hosts for upgrade 1 at a time \u001b[1;33r\u001b[32;1H"]
[641.19346, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[36mINFO\u001b[39m[0146] Processing controlplane host 10.3.3.151      \r\n\u001b[36mINFO\u001b[39m[0146] [controlplane] Now checking status of node 10.3.3.151, try #1 \u001b[1;33r\u001b[32;1H"]
[666.209845, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[31mERRO\u001b[39m[0171] Failed to upgrade hosts: 10.3.3.151 with error [[controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found] \u001b[1;33r\u001b[32;1H"]
[666.210877, "o", "\u001b[1;32r\u001b[2S\u001b[30d\u001b[31mFATA\u001b[39m[0171] [controlPlane] Failed to upgrade Control Plane: [[[controlplane] Error getting node 10.3.3.151:  \"10.3.3.151\" not found]] \u001b[1;33r\u001b[32;1H"]
[666.225196, "o", "\u001b[32m\u001b[1maida@masterk8s\u001b[m\u000f:\u001b[34m\u001b[1m~\u001b[m\u000f$ "]
[670.700129, "o", "\u001b[?25l\u001b[32m\u001b[40m\r\n[AIDA] 1:AIDA-Cloud*Z 2:AIDA-Edge-                                                                \"masterk8s\" 18:38 05-Feb-24\u001b[m\u000f\u001b[32;19H\u001b[34h\u001b[?25h"]
[686.417504, "o", "\u001b[1;33r\u001b[m\u000f\u001b[?1l\u001b>\u001b[H\u001b[J\u001b[34h\u001b[?25h\u001b[?1000l\u001b[?1002l\u001b[?1006l\u001b[?1005l\u001b[?1049l"]
[686.417987, "o", "[detached (from session AIDA)]\r\n"]
